# üöÄ Universal DataLoader

**AI-Powered Knowledge Graph Construction for Enterprise Infrastructure**

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![LangChain](https://img.shields.io/badge/LangChain-0.3+-green.svg)](https://python.langchain.com/)
[![Neo4j](https://img.shields.io/badge/Neo4j-5.0+-red.svg)](https://neo4j.com/)
[![Tests](https://img.shields.io/badge/tests-passing-brightgreen.svg)](#testing)

Transform unstructured infrastructure data into intelligent knowledge graphs using **LangChain** and **Neo4j**. Production-ready foundation for **Graph RAG**, **infrastructure analysis**, and **intelligent operations**.

---

## üìñ Table of Contents

- [üéØ Overview](#-overview)
- [‚ö° Quick Start](#-quick-start)
- [üìã Prerequisites](#-prerequisites)
- [üõ†Ô∏è Installation](#Ô∏è-installation)
- [‚öôÔ∏è Configuration](#Ô∏è-configuration)
- [üöÄ Usage](#-usage)
- [üìñ API Reference](#-api-reference)
- [üß™ Testing](#-testing)
- [ü§ù Contributing](#-contributing)
- [üêõ Troubleshooting](#-troubleshooting)
- [üìä Performance](#-performance)
- [üîß Technical Deep Dive](#-technical-deep-dive)

---

## üéØ Overview

### **What Problem Does This Solve?**

Enterprise organizations struggle to extract **actionable intelligence** from massive amounts of unstructured infrastructure data (logs, configs, documents, incident reports). Traditional parsing creates **disconnected data points**‚Äîthis solution creates **intelligent, queryable knowledge graphs**.

### **Core Value Proposition**

```python
# INPUT: Raw infrastructure files
/var/log/secure, /etc/httpd/conf/httpd.conf, /var/log/yum.log...

# PROCESSING: AI semantic understanding
LangChain LLMGraphTransformer + LLM API ‚Üí Intelligent entity extraction

# OUTPUT: Connected knowledge graph  
(web-prod-01:Server)-[:RUNS]->(httpd:Service)
(httpd:Service)-[:DEPENDS_ON]->(openssl:Package)
(CVE-2023-12345:Vulnerability)-[:AFFECTS]->(openssl:Package)

# CAPABILITY: Graph RAG queries
"Which production servers are affected by the latest OpenSSL vulnerability?"
```

### **Key Benefits**

- **üß† AI-Powered**: Semantic understanding via LLMs, not brittle regex patterns
- **üîó Intelligent Relationships**: Auto-discovers complex dependencies and connections  
- **üéØ Graph RAG Ready**: Perfect foundation for intelligent retrieval systems
- **üåç Domain Agnostic**: Works with IT infrastructure, security, business processes, documents
- **‚ö° Production Scale**: Handles enterprise workloads (1000+ systems, 15K+ entities)
- **üîí Enterprise Ready**: Security, configuration management, monitoring integration

---

## ‚ö° Quick Start

**Get a working knowledge graph in 15 minutes:**

```bash
# 1. Clone and setup environment
git clone https://github.com/rrbanda/dataloader.git
cd dataloader
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 2. Setup Neo4j Desktop (required)
# Download: https://neo4j.com/download/
# Create database "dataloader-db" with password "password"
# Install APOC plugin

# 3. Configure API access
export OPENAI_API_KEY="your-llm-api-key"
export OPENAI_BASE_URL="https://llama-4-scout-17b-16e-w4a16-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1"
export MODEL="llama-4-scout-17b-16e-w4a16"
export NEO4J_URI="neo4j://127.0.0.1:7687"
export NEO4J_USERNAME="neo4j"
export NEO4J_PASSWORD="password"

# 4. Generate test data and verify setup
python utils/rhel_filesystem_generator.py
python test_setup.py  # Should show 5/5 tests pass

# 5. Create your first knowledge graph
python -c "
from core.unified_dataloader import get_universal_loader
loader = get_universal_loader()
systems, events = loader.load_all_systems()
print(f' Knowledge graph created: {len(systems)} systems processed')
loader.close()
"

# 6. Explore in Neo4j Desktop Browser
# Query: MATCH (n)-[r]->(m) RETURN n, r, m LIMIT 25
```

**Expected Result:** Interactive knowledge graph with ~39 nodes and 16 relationships representing intelligent infrastructure analysis.

---

## üìã Prerequisites

### **Required Software**
- **Python 3.9+** ([Download](https://www.python.org/downloads/))
- **Neo4j Desktop** ([Download](https://neo4j.com/download/)) - Required for graph storage
- **Git** ([Download](https://git-scm.com/downloads))

### **Required API Access**
- **LLM API Key** (Red Hat AI: [Get key](https://ai.redhat.com/), OpenAI: [Get key](https://platform.openai.com/api-keys))
  - Create account with LLM provider ‚Üí API Keys section ‚Üí Create new key

### **System Requirements**
- **Memory**: 4GB minimum, 8GB recommended for large datasets
- **Storage**: 2GB free space
- **Network**: Internet access for AI API calls

---

## üõ†Ô∏è Installation

### **Standard Installation**

```bash
# Clone repository
git clone https://github.com/rrbanda/dataloader.git
cd dataloader

# Create isolated environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# Verify installation
python -c "import langchain, neo4j; print(' Dependencies installed')"
```

### **Neo4j Desktop Setup**

1. **Install Neo4j Desktop** ‚Üí [Download here](https://neo4j.com/download/)
2. **Create Project** ‚Üí "dataloader-project"  
3. **Add Database**:
   - Name: `dataloader-db`
   - Password: `password`
   - Version: Latest 5.x
4. **Install APOC Plugin** ‚Üí Select database ‚Üí Plugins ‚Üí APOC ‚Üí Install
5. **Start Database** ‚Üí Click ‚ñ∂Ô∏è button

**Verify Neo4j:**
```bash
lsof -i :7687  # Should show Neo4j process
```

---

## ‚öôÔ∏è Configuration

### **Environment Variables**

```bash
# LLM Configuration
export OPENAI_API_KEY="your-llm-api-key"
export OPENAI_BASE_URL="https://llama-4-scout-17b-16e-w4a16-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1"
export MODEL="llama-4-scout-17b-16e-w4a16"

# Neo4j Configuration
export NEO4J_URI="neo4j://127.0.0.1:7687"
export NEO4J_USERNAME="neo4j"
export NEO4J_PASSWORD="password"
export NEO4J_DATABASE="neo4j"
```

### **Configuration File: `config/data_loader_config.yaml`**

```yaml
# Data source configuration
data_sources:
  primary_data:
    type: "filesystem"
    base_path: "simulated_rhel_systems"
    file_patterns:
      system_info: ["**/system_info.txt"]
      logs: ["**/*.log"]
      configs: ["**/*.conf", "**/*.yaml"]

# LLM configuration (environment-driven)
llm_config:
  enabled: true

# Neo4j configuration (environment-driven)
neo4j_config:
  uri: "${NEO4J_URI}"
  username: "${NEO4J_USERNAME}"
  password: "${NEO4J_PASSWORD}"
  database: "${NEO4J_DATABASE}"
```

---

## üè¢ Enterprise Integration

### **Works WITH Your Existing Infrastructure**

The Universal DataLoader is designed to **complement, not replace** your existing enterprise tools. Here's how it integrates with common infrastructure:

#### **ü§ù No Conflicts with Existing Tools**

```bash
# Your existing infrastructure (unchanged)
Red Hat Insights    ‚îÄ‚îÄ‚Üí Security recommendations & compliance
Splunk/ELK         ‚îÄ‚îÄ‚Üí Centralized logging & search  
Prometheus/Grafana ‚îÄ‚îÄ‚Üí Metrics monitoring & alerting
rsyslog/fluentd    ‚îÄ‚îÄ‚Üí Log collection & forwarding
SIEM tools         ‚îÄ‚îÄ‚Üí Security event correlation

# Universal DataLoader adds:
Knowledge Graph    ‚îÄ‚îÄ‚Üí AI-powered infrastructure intelligence
Graph RAG          ‚îÄ‚îÄ‚Üí Intelligent operations & queries
```

#### **üìñ Read-Only Data Access**

The dataloader **only reads** existing files - it never modifies or interferes:

```python
# What it reads (read-only access):
/var/log/messages      # ‚úÖ Standard syslog files
/var/log/secure        # ‚úÖ SSH authentication logs  
/var/log/yum.log       # ‚úÖ Package installation logs
/etc/redhat-release    # ‚úÖ System version info
/proc/cpuinfo         # ‚úÖ Hardware information

# What it NEVER touches:
- Log collection configurations (rsyslog.conf)
- Red Hat Insights client settings  
- Active databases or services
- Network configurations
- Security policies
```

### **üîÑ Integration Scenarios**

#### **Scenario 1: Alongside Red Hat Insights**
```python
# Complementary intelligence
Red Hat Insights:      SaaS security analysis & recommendations
Universal DataLoader:  Local knowledge graph & Graph RAG

# Combined benefits:
insights_data = insights_api.get_vulnerabilities()
local_graph = dataloader.create_knowledge_graph()
intelligent_ops = combine_insights_with_graph_rag(insights_data, local_graph)
```

#### **Scenario 2: With Centralized Logging (Splunk/ELK)**
```python
# Enhanced log intelligence  
Splunk/ELK:           Centralized log storage & search
Universal DataLoader: AI-powered relationship extraction

# Graph RAG queries become possible:
"Which servers have both SSH failures AND vulnerable packages?"
"Show dependency chain for services affected by security patches"
```

#### **Scenario 3: With Monitoring Stack (Prometheus/Grafana)**
```python
# Infrastructure intelligence
Prometheus/Grafana:   Metrics monitoring & dashboards
Universal DataLoader: Semantic understanding of infrastructure

# Intelligent correlation:
"Which high-CPU systems also have recent package vulnerabilities?"
"Show service dependencies for systems with memory alerts"
```

### **üéØ Production Deployment Integration**

#### **Minimal Infrastructure Impact**
```yaml
# Resource requirements
CPU Usage:    Low - only during batch processing
Memory:       2-4GB during knowledge graph creation  
I/O Impact:   Read-only file access, no write operations
Network:      LLM API calls only (configurable endpoints)
Storage:      No additional storage on monitored systems
```

#### **Security & Compliance**
```bash
# Maintains enterprise security standards
Data Access:     Read-only filesystem access
API Keys:        Externalized configuration (environment variables)
Audit Trail:     All operations logged
Network:         Configurable endpoints (on-premise LLM support planned)
Encryption:      Data in transit encrypted (HTTPS/TLS)
```

### **üöÄ Future Integration Roadmap**

#### **Phase 1: API Integrations (Q2 2024)**
```yaml
# Direct API consumption (no file system access)
red_hat_insights_api:
  vulnerabilities: "/api/insights/v1/vulnerabilities"
  compliance: "/api/insights/v1/compliance" 
  recommendations: "/api/insights/v1/advisor"

splunk_api:
  search: "/services/search/jobs"
  saved_searches: "/services/saved/searches"
  
elasticsearch_api:
  search: "/_search"
  indices: "/_cat/indices"
```

#### **Phase 2: Enterprise Connectors (Q3 2024)**
```yaml
# Native integrations
satellite_connector:
  systems_inventory: "satellite.example.com/api/v2/hosts"
  package_management: "satellite.example.com/api/v2/packages"

prometheus_connector:
  metrics_query: "prometheus.example.com/api/v1/query"
  alert_rules: "prometheus.example.com/api/v1/rules"
```

#### **Phase 3: Enterprise Features (Q4 2024)**
```yaml
# Enterprise-grade capabilities  
rbac_integration:
  active_directory: "LDAP/AD authentication"
  role_based_access: "Fine-grained permissions"
  
audit_compliance:
  sox_compliance: "Financial audit trails"
  hipaa_compliance: "Healthcare data protection"
  gdpr_compliance: "Data privacy controls"
```

### **üìã Integration Checklist**

#### **Before Deployment**
- [ ] **Identify data sources**: Which logs/configs to analyze
- [ ] **Verify permissions**: Read-only access to target files  
- [ ] **Network access**: LLM API endpoints reachable
- [ ] **Resource planning**: 4-8GB RAM for processing
- [ ] **Security review**: API key management strategy

#### **During Deployment**  
- [ ] **Start with test data**: Use simulated systems first
- [ ] **Validate knowledge graph**: Check entity extraction quality
- [ ] **Test Graph RAG queries**: Verify intelligent responses
- [ ] **Monitor resource usage**: CPU, memory, network impact
- [ ] **Security audit**: Review API key security

#### **Post-Deployment**
- [ ] **Integration testing**: Verify compatibility with existing tools
- [ ] **Performance monitoring**: Track processing times and accuracy
- [ ] **User training**: Enable teams to use Graph RAG capabilities  
- [ ] **Expand data sources**: Add more systems incrementally
- [ ] **Plan API integrations**: Connect to enterprise APIs

### **üí° Best Practices for Enterprise Integration**

#### **1. Phased Rollout Strategy**
```bash
# Recommended deployment approach
Phase 1: Development ‚Üí Test with simulated data
Phase 2: Staging    ‚Üí Small subset of real systems  
Phase 3: Production ‚Üí Full enterprise deployment
Phase 4: Integration ‚Üí Connect to enterprise APIs
```

#### **2. Data Source Strategy**
```bash
# Start with low-risk, high-value data
Tier 1: System info files (static data)
Tier 2: Historical logs (archived data)  
Tier 3: Current logs (operational data)
Tier 4: Real-time streams (future capability)
```

#### **3. Security & Governance**
```bash
# Enterprise security alignment
Data Classification: Treat as internal/confidential
API Key Management: Use enterprise secret management
Access Controls: Implement least-privilege access
Audit Requirements: Log all data access operations
```

---

## üöÄ Usage

### **Basic Knowledge Graph Creation**

```python
from core.unified_dataloader import get_universal_loader

# Create and load systems
loader = get_universal_loader()
systems, events = loader.load_all_systems()

print(f"Created knowledge graph with {len(systems)} systems")
loader.close()
```

### **Custom Data Sources**

```python
# Point to your infrastructure data
config = {
    "data_sources": {
        "your_data": {
            "type": "filesystem",
            "base_path": "/path/to/your/logs",
            "file_patterns": {
                "logs": ["**/*.log", "**/*.txt"],
                "configs": ["**/*.conf", "**/*.yaml"]
            }
        }
    }
}

loader = get_universal_loader(config=config)
systems, events = loader.load_all_systems()
```

### **Querying the Knowledge Graph**

```python
# Execute intelligent Cypher queries
result = loader.neo4j_graph.query("""
    MATCH (s:Server)-[:RUNS]->(svc:Service)
    WHERE s.environment = 'production'
    RETURN s.name, svc.name, svc.status
""")

for record in result:
    print(f"Server: {record['s.name']}, Service: {record['svc.name']}")
```

### **Scale Testing with Generated Data**

```python
from utils.rhel_filesystem_generator import RHELFilesystemGenerator

# Generate enterprise-scale test data
generator = RHELFilesystemGenerator(num_systems=100)
generator.generate_all_systems()
print(" Generated 100 enterprise RHEL systems")
```

---

## üìñ API Reference

### **Core Classes**

#### **UnifiedDataLoader**
```python
class UnifiedDataLoader:
    def load_all_systems(self) -> Tuple[List[SystemEntity], List[EventEntity]]
        """Load all systems and create knowledge graph"""
        
    def load_system(self, system_id: str) -> Tuple[SystemEntity, List[EventEntity]]
        """Load specific system"""
        
    def close(self) -> None
        """Cleanup connections"""
```

#### **RHELFilesystemGenerator**
```python
class RHELFilesystemGenerator:
    def __init__(self, num_systems: int = 5)
    def generate_all_systems(self) -> Dict[str, Dict]
        """Generate realistic RHEL systems"""
```

### **Configuration Classes**

#### **UnifiedConfigLoader**
```python
class UnifiedConfigLoader:
    def get_llm_config(self) -> Dict[str, Any]
    def get_neo4j_config(self) -> Dict[str, Any]
    def get_dataloader_config(self) -> Dict[str, Any]
```

### **Data Models**

```python
@dataclass
class SystemEntity:
    system_id: str
    hostname: str
    environment: str
    services: List[str]
    metadata: Dict[str, Any]

@dataclass  
class EventEntity:
    event_id: str
    system_id: str
    event_type: str
    timestamp: datetime
    description: str
    metadata: Dict[str, Any]
```

---

## üß™ Testing

### **Run Test Suite**

```bash
# Activate environment
source .venv/bin/activate

# Run all tests
python -m pytest tests/ -v

# Run with coverage
python -m pytest tests/ --cov=core --cov=config --cov=utils

# Quick setup verification  
python test_setup.py  # Should show 5/5 tests pass
```

### **Integration Testing**

```bash
# Test with generated data
python utils/rhel_filesystem_generator.py 10
python tests/test_complete_4phase_pipeline.py
```

### **Performance Testing**

```bash
# Benchmark with scale data
python utils/rhel_filesystem_generator.py 1000
time python -c "
from core.unified_dataloader import get_universal_loader
loader = get_universal_loader()
systems, events = loader.load_all_systems()
print(f'Processed {len(systems)} systems')
loader.close()
"
```

---

## ü§ù Contributing

We welcome contributions! Please follow these guidelines:

### **Development Setup**

```bash
# Fork repo, then:
git clone https://github.com/YOUR-USERNAME/dataloader.git
cd dataloader

# Setup development environment
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Create feature branch
git checkout -b feature/your-feature
```

### **Contribution Process**

1. **üç¥ Fork** the repository
2. **üåø Create feature branch** from `main`
3. ** Add tests** for new functionality
4. **üß™ Ensure tests pass**: `python -m pytest tests/ -v`
5. **üìù Update documentation** as needed
6. **üí´ Create pull request** with clear description

### **Code Standards**

- **Style**: Follow PEP 8, use `black` formatter
- **Tests**: Maintain >90% test coverage
- **Documentation**: Update README and docstrings
- **Commits**: Use conventional commit format

### **Areas for Contribution**

- üîå **Data Source Adapters**: APIs, databases, cloud storage
- üéØ **Domain Templates**: Healthcare, finance, manufacturing
- üß™ **Testing**: Performance tests, edge cases
- üìö **Documentation**: Tutorials, examples, guides
- ‚ö° **Performance**: Optimization, parallel processing

---

## üêõ Troubleshooting

### **Common Issues**

#### **Neo4j Connection Problems**
```bash
# Check if Neo4j is running
lsof -i :7687

# Verify credentials
echo $NEO4J_PASSWORD  # Should be 'password'
```

#### **LLM API Issues**
```bash
# Verify API configuration
echo $OPENAI_API_KEY
echo $OPENAI_BASE_URL

# Test connection
python -c "from langchain_openai import ChatOpenAI; print(' LLM working')"
```

#### **Data Loading Issues**
```bash
# Generate sample data
python utils/rhel_filesystem_generator.py

# Check file permissions
ls -la simulated_rhel_systems/
```

### **Performance Issues**

- **Slow processing**: Reduce batch size, check network latency
- **Memory errors**: Increase system memory or reduce dataset size
- **Neo4j issues**: Ensure APOC plugin installed, check database status

### **Getting Help**

1. **üìñ Check documentation** and troubleshooting section
2. **üîç Search existing issues** on GitHub
3. **üêõ Create detailed issue** with environment details and logs
4. **üí¨ Join discussions** for questions and community support

---

## üìä Performance

### **Benchmarks**

| Systems | Processing Time | Entities Created | Memory Usage |
|---------|----------------|------------------|--------------|
| 5       | 45 seconds     | ~45 entities     | 1.2GB        |
| 50      | 8 minutes      | ~450 entities    | 2.1GB        |
| 100     | 15 minutes     | ~900 entities    | 2.8GB        |
| 1000    | 3.5 hours      | ~9000 entities   | 4.2GB        |

### **Limitations**

- **Data Sources**: Filesystem only (APIs/databases planned)
- **File Size**: Large files (>50MB) may need chunking
- **Languages**: Optimized for English text
- **LLM Dependency**: Requires internet connection for AI processing

### **Scalability**

- **Maximum tested**: 1000 systems (15K entities, 12K relationships)
- **Rate limits**: Depends on LLM provider (~100 requests/minute)
- **Hardware requirements**: 8GB RAM recommended for 1000+ systems

---

## üîß Technical Deep Dive

### **Why LLMs Are Essential**

Traditional log parsing uses **rigid patterns**:
```bash
# Regex approach (limited)
"httpd\[(\d+)\]: (.+)" ‚Üí Extract PID and message
#  Misses context, relationships, semantic meaning
```

**LLMs provide semantic intelligence**:
```python
# AI approach (intelligent)
"systemd[1]: Started The Apache HTTP Server"
#  Understands: Apache = httpd service
#  Infers: systemd MANAGES httpd
#  Connects: Related to ports 80/443, SSL, web traffic
```

### **AI Entity Extraction Process**

#### **Input: RHEL System Files**
```
system/
‚îú‚îÄ‚îÄ /etc/redhat-release     ‚Üí Server entity (version, architecture)
‚îú‚îÄ‚îÄ /var/log/messages       ‚Üí Service events, system activities
‚îú‚îÄ‚îÄ /var/log/secure         ‚Üí User activities, authentication
‚îú‚îÄ‚îÄ /var/log/yum.log        ‚Üí Package installations/updates
‚îú‚îÄ‚îÄ /etc/httpd/conf/        ‚Üí Service configurations
‚îî‚îÄ‚îÄ /var/lib/insights/      ‚Üí Security findings, vulnerabilities
```

#### **AI Recognition Patterns**

**üñ•Ô∏è Server Entities:**
```python
# From: /etc/redhat-release
"Red Hat Enterprise Linux release 9.3"
# AI extracts ‚Üí
Server {
    name: "web-prod-01",
    rhel_version: "9.3",
    environment: "production"
}
```

**‚öôÔ∏è Service Entities:**
```python  
# From: /var/log/messages
"systemd[1]: Started The Apache HTTP Server"
# AI extracts ‚Üí
Service {
    name: "httpd",
    status: "active",
    managed_by: "systemd"
}
```

**üîó Intelligent Relationships:**
```cypher
# AI automatically creates:
(web-prod-01:Server)-[:RUNS]->(httpd:Service)
(httpd:Service)-[:USES]->(httpd-package:Package)
(httpd:Service)-[:DEPENDS_ON]->(openssl:Package)
```

### **Graph RAG Capabilities**

With LLM-extracted graphs, you can ask:
```python
"Which production web servers have SSL configuration issues?"
"Show me dependency chains for services with recent security patches"
"What would be impacted if I restart the MySQL service?"
```

The AI creates **semantic relationships** that enable sophisticated queries impossible with traditional parsing.

---

## üìÑ License

Licensed under the **Apache License 2.0** - see [LICENSE](LICENSE) file.

**Summary**:  Commercial use, modification, distribution allowed. ‚ùó License notice required.

---

## üôè Acknowledgments

**Built With:**
- **[LangChain](https://python.langchain.com/)** - LLM application framework
- **[Neo4j](https://neo4j.com/)** - Graph database platform
- **[Red Hat AI](https://ai.redhat.com/)** / **[OpenAI](https://openai.com/)** - LLM API services

**Contributors:**
- **[@rrbanda](https://github.com/rrbanda)** - Creator and maintainer
- **Community** - See [Contributors](https://github.com/rrbanda/dataloader/graphs/contributors)

---

**üåü Star this project if it helps you build intelligent infrastructure graphs!**

**üì¨ Questions? Open an [issue](https://github.com/rrbanda/dataloader/issues) or [discussion](https://github.com/rrbanda/dataloader/discussions).**